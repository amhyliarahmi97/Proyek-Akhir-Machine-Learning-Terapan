# -*- coding: utf-8 -*-
"""Proyek_Akhir_Machine_Learning_Terapan3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aoYArheRIgdMJfuASE5FUSC-D3OAHaOD

# ðŸ“˜ Laporan Proyek: Sistem Rekomendasi Buku

---
By : Rahmi Amilia

Proyek ini bertujuan membangun sistem rekomendasi buku berbasis dua pendekatan: Collaborative Filtering (SVD) dan Content-Based Filtering. Dataset yang digunakan adalah Book-Crossing Dataset yang terdiri dari informasi buku, pengguna, dan rating.

Notebook ini mencakup tahapan: import library, load data, data understanding, data preparation, modeling, dan evaluasi model.

### 1. Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix

import warnings
warnings.filterwarnings("ignore")

"""### 2. Load Dataset

Memuat data dari file CSV Book-Crossing ke dalam DataFrame: books, ratings, dan users.
"""

books = pd.read_csv('/content/BX_Books.csv', sep=';', encoding='latin-1', nrows=10000)
ratings = pd.read_csv("/content/BX-Book-Ratings.csv", sep=';', encoding='latin-1', on_bad_lines='skip')
users = pd.read_csv("/content/BX-Users.csv", sep=';', encoding='latin-1', on_bad_lines='skip')

"""### 3. Data Understanding

Dataset terdiri dari:
- books.csv: 10.000 baris, 8 kolom
- ratings.csv: 1.149.780 baris, 3 kolom
- users.csv: 278.858 baris, 3 kolom

Fitur penting:
- `isbn`, `title`, `author`, `year`, `publisher`, `image_s/m/l`
- `user_id`, `location`, `age`
- `rating` (0-10)

> Kolom `age` memiliki missing values dan outlier seperti usia <5 atau >100.

Berikut adalah jumlah baris dan kolom pada masing-masing dataset sebelum dilakukan proses pembersihan:
"""

print('jumlah data books : ', books.shape)
print('jumlah data ratings : ', ratings.shape)
print('jumlah data users : ', users.shape)

"""Dataset `ratings` terdiri dari 3 kolom utama:

- **user_id**: ID unik dari pengguna yang memberikan rating
- **isbn**: ID unik dari buku yang diberi rating
- **rating**: Nilai rating yang diberikan oleh pengguna terhadap buku (skala 0â€“10)
"""

ratings.columns = ["user_id", "isbn", "rating"]
ratings.head()

"""Dataset `books` memiliki 8 kolom, yaitu:

- **isbn**: ID unik buku (International Standard Book Number)
- **title**: Judul buku
- **author**: Nama penulis buku
- **year**: Tahun terbit buku
- **publisher**: Nama penerbit
- **image_s**, **image_m**, **image_l**: URL gambar sampul buku dalam ukuran kecil, sedang, dan besar

"""

books.columns = ["isbn", "title", "author", "year", "publisher", "image_s", "image_m", "image_l"]
books.head()

"""Dataset `users` terdiri dari 3 kolom utama:

- **user_id**: ID unik dari masing-masing pengguna
- **location**: Lokasi pengguna, biasanya berupa format "kota, negara bagian/provinsi, negara"
- **age**: Usia pengguna (dalam tahun), namun mengandung banyak nilai yang hilang (NaN)

"""

users.columns = ["user_id", "location", "age"]
users.head()

"""### Informasi Struktur dan Statistik Dataset Books

#### a. Struktur Data (`books.info()`)

- Dataset `books` memiliki **10.000 baris** dan **8 kolom**.
- Semua kolom tidak memiliki nilai null (non-null count = 10000).
- Tipe data:
  - 1 kolom bertipe numerik (`year`)
  - 7 kolom bertipe objek (string), termasuk `isbn`, `title`, `author`, dan kolom gambar

#### b. Statistik Deskriptif (`books.describe(include='all')`)

- Kolom **`title`** memiliki 9.553 nilai unik â†’ artinya ada beberapa judul yang berulang.
- Kolom **`author`** memiliki 5.754 nama unik, dengan penulis paling sering muncul adalah **Stephen King** sebanyak **68 kali**.
- Kolom **`publisher`** memiliki 1.701 penerbit unik, dan yang paling sering muncul adalah **Ballantine Books**.
- Kolom **`year`** memiliki nilai minimum 0 dan maksimum 2005, dengan rata-rata sekitar tahun **1958**. Nilai tahun 0 merupakan outlier dan akan difilter pada tahap data preparation.
- Kolom gambar (`image_s`, `image_m`, `image_l`) hampir seluruhnya unik, menunjukkan setiap buku memiliki URL gambar tersendiri.

"""

books.info()
books.describe(include='all')

users.info()
users.describe()

"""Kolom-kolom pada dataset ratings:

* user_id : ID pengguna.

* isbn : ID buku.

* rating : Nilai rating dari pengguna terhadap buku. Nilai 0 menunjukkan rating implisit (belum memberikan penilaian eksplisit).
"""

ratings.info()
ratings['rating'].value_counts().sort_index()

"""### Cek Missing Values dan Statistik Usia"""

books.columns = ['isbn', 'title', 'author', 'year', 'publisher', 'image_s', 'image_m', 'image_l']
ratings.columns = ['user_id', 'isbn', 'rating']
users.columns = ['user_id', 'location', 'age']
print(users.isnull().sum())
print(users['age'].describe())

"""### 4. Data Preparation

### 4.1 Rename Kolom
4.2 Hapus Duplikat ISBN
"""

books.drop_duplicates(subset='isbn', keep='first', inplace=True)

"""4.3 Filter Buku dengan Tahun Tidak Masuk Akal"""

books = books[(books['year'] >= 1900) & (books['year'] <= 2025)]

"""4.4 Hapus Kolom Gambar"""

books.drop(columns=['image_s', 'image_m', 'image_l'], inplace=True)

"""4.5 Filter Rating Eksplisit"""

ratings = ratings[ratings['rating'] > 0]

"""e. Isi nilai kosong dan gabungkan fitur"""

books['title'] = books['title'].fillna('')
books['author'] = books['author'].fillna('')
books['combined_features'] = books['title'] + ' ' + books['author']

"""f. TF-IDF Vectorization untuk Content-Based Filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(books['combined_features'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

"""### 5. Modeling & Results

5.1 Collaborative Filtering (SVD)

* Menggunakan Surprise library untuk menerapkan SVD.

* Melatih model menggunakan data eksplisit.

* Memberikan rekomendasi Top-N buku untuk user tertentu.
"""

!pip install scikit-surprise --no-binary scikit-surprise

!pip install numpy==1.23.5

from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise.accuracy import rmse

reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(ratings[['user_id', 'isbn', 'rating']], reader)

trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

svd_model = SVD()
svd_model.fit(trainset)

predictions = svd_model.test(testset)
rmse(predictions)

"""5.2 Contoh Output Rekomendasi SVD

Membuat fungsi rekomendasi untuk user tertentu:
"""

def recommend_books_svd(user_id, books_df, ratings_df, model, n=5):
    user_books = ratings_df[ratings_df['user_id'] == user_id]['isbn'].tolist()
    unread_books = books_df[~books_df['isbn'].isin(user_books)]

    predictions = []
    for isbn in unread_books['isbn']:
        pred = model.predict(user_id, isbn)
        predictions.append((isbn, pred.est))

    top_n = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]
    return books_df[books_df['isbn'].isin([isbn for isbn, _ in top_n])][['title', 'author']]

recommend_books_svd(user_id=276729, books_df=books, ratings_df=ratings, model=svd_model)

"""5.3 Content-Based Filtering

* Gunakan fitur title dan author dari buku.

* Ubah menjadi vektor TF-IDF.

* Hitung Cosine Similarity antar buku.

* Berikan rekomendasi untuk buku tertentu.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

books['title'] = books['title'].fillna('')
books['author'] = books['author'].fillna('')

books['combined_features'] = books['title'] + ' ' + books['author']

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(books['combined_features'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

"""Contoh Output Rekomendasi Content-Based"""

def get_recommendations(title, books_df, cosine_sim, n=5):
    from difflib import get_close_matches
    import numpy as np
    import pandas as pd
    indices = pd.Series(books_df.index, index=books_df['title']).drop_duplicates()
    if title not in indices:
        closest = get_close_matches(title, books_df['title'], n=1)
        if not closest:
            return "Judul tidak ditemukan, dan tidak ada kemiripan yang cukup."
        else:
            title = closest[0]
            print(f"Menampilkan hasil rekomendasi untuk judul terdekat: '{title}'")
    idx = indices[title]
    sim_vector = cosine_sim[idx]
    if hasattr(sim_vector, 'toarray'):
        sim_vector = sim_vector.toarray().flatten()
    else:
        sim_vector = np.array(sim_vector).flatten()
    sim_scores = list(enumerate(sim_vector))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n+10]
    book_indices = [i[0] for i in sim_scores if i[0] < len(books_df)]
    return books_df.iloc[book_indices[:n]][['title', 'author']]

get_recommendations("Harry Potter and the Chamber of Secrets (Book 2)", books, cosine_sim)

from difflib import get_close_matches
get_close_matches("Harry Potter and the Chamber of Secrets", books['title'], n=5)

"""### 6. Evaluation

6.1 Evaluasi Collaborative Filtering (SVD)

Metrik: RMSE (Root Mean Squared Error)
RMSE mengukur seberapa jauh hasil prediksi model terhadap nilai rating aktual. Nilai RMSE lebih kecil berarti lebih baik.
"""

from surprise import accuracy

rmse_score = accuracy.rmse(predictions)

"""Metrik Tambahan: Precision@K dan Recall@K

Metrik ini digunakan untuk mengevaluasi seberapa relevan hasil rekomendasi bagi user. Kita gunakan K = 5.
"""

from collections import defaultdict

def precision_recall_at_k(predictions, k=5, threshold=7):
    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = {}
    recalls = {}

    for uid, user_ratings in user_est_true.items():
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        top_k = user_ratings[:k]

        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)
        n_rec_k = sum((est >= threshold) for (est, _) in top_k)
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in top_k)

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0

    avg_precision = sum(prec for prec in precisions.values()) / len(precisions)
    avg_recall = sum(rec for rec in recalls.values()) / len(recalls)

    return avg_precision, avg_recall

precision, recall = precision_recall_at_k(predictions, k=5)
print(f"Precision@5: {precision:.4f}")
print(f"Recall@5: {recall:.4f}")